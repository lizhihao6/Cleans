<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title> Cleans Project </title>
    <link rel="stylesheet" href="resources/style.css">
    <link href="https://fonts.googleapis.com/css?family=Raleway:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lato:400,700,700italic,400italic" rel="stylesheet" type="text/css">
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
    <script src="resources/js/modernizr.js"></script>
    <meta name="viewport" content="width=device-width">
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
	//   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	//   (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	//   m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	//   })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	
	//   ga('create', 'UA-105303874-1', 'auto');
	//   ga('send', 'pageview');
    </script>
</head>

<!--
<button id="demo-button" class="online-demo" onclick="open_demo();"><img src="assets/img/demo_button2.png"></button> 

<div id="demos" style="display:none">
	<a href="#" style="position: fixed; right: 2.2vmax; top: 6.3vw; color: white; z-index:9999; text-decoration: none; font-size: 1.2vmax;" onclick="close_demo()"> &#10006; </a>
	<iframe class="demo" src="http://quality-gpu.eastus.cloudapp.azure.com:8080" name="targetframe" allowTransparency="true" scrolling="yes" frameborder="0" >
	<!--<iframe class="demo" src="http://127.0.0.1:5000/" name="targetframe" allowTransparency="true" scrolling="yes" frameborder="0" >
	</iframe>
</div>
-->

<body data-spy="scroll" data-offset="0" data-target="#navbar-main">

<!-- ==== MENU ==== -->

<nav>
	<!-- <ul class="left_bar">
		<a href="index.html"> <li> DPED </li></a>
		<a href="wespe.html"> <li> WESPE </li></a>
		<a href="#title"> <li> PyNET </li></a>
	</ul> -->
	<!-- <ul class="left_bar"> -->
		<!-- <a href="index.html"> <li2> DPED </li2></a>
		<a href="wespe.html"> <li2> WESPE </li2></a>
		<a href="#title"> <li2> PyNET </li2></a>
		<a href="pynet-bokeh.html"> <li2 style="background-color: rgba(233,79,112, 0.85);"> BOKEH </li2></a> -->
	<!-- </ul> -->
	<ul class="middle_bar">
		<a href="#title"> <li> PAPER </li></a>
		<a href="#demo"> <li> DEMO </li></a>
		<a href="#method"> <li> METHOD </li></a>
		<a href="#dataset"> <li> DATASET </li></a>
		<!-- <a href="#algo"> <li> ALGO </li></a> -->
		<a href="#code"> <li> CODE </li></a>
	</ul>
	<!-- <ul class="right_bar">
		<a target="_blank" href="http://www.vision.ee.ethz.ch/en/" > <li> <img src="assets/img/eth_logo.png"> </li></a>
		<a href="#about"> <li> ABOUT </li></a>
	</ul> -->
</nav>

<div id="title">
<h1 class="title">
	<!-- <span style="color:red;">CLEANS</span> -->
	<!-- -- -->
	<span style="color:red;">C</span>ompress C<span style="color:red;">le</span>an Sign<span style="color:red;">a</span>l from <span style="color:red;">N</span>oisy Raw Image: A <span style="color:red;">S</span>elf-Supervised Approach
</h1>
	  
<table class="authors-pynet">
<tr>
	<td><a target="_blank" class = "link-invisible" href="https://lizhihao6.github.io/">Zhihao Li*</a></td>
	<td><a target="_blank" class = "link-invisible" href="https://wyf0912.github.io/">Yufei Wang*</a></td>
	<td><a target="_blank" class = "link-invisible" href="https://scholar.google.com/citations?user=UGZXLxIAAAAJ&hl=en">Alex Kot</a></td>
	<td><a target="_blank" class = "link-invisible" href="https://scholar.google.com/citations?user=ypkClpwAAAAJ&hl=en">Bihan Wen</a></td>
</tr>
<tr class="emails">
	<td>zhihao.li [AT] ntu.edu.sg</td>
	<td>yufei001 [AT] e.ntu.edu.sg</td>
	<td>eackot [AT] ntu.edu.sg</td>
	<td>bihan.wen [AT] ntu.edu.sg</td>
</tr>
</td>
</table>

<h1 class="institute"><a target="_blank" href="https://icml.cc/virtual/2024/poster/34960"> ICML 2024 Paper </a></h1>

</div>

<div id="abstract">
	<div>
		<div class="title-arxiv-pynet">
	       	<p><a target="_blank" href="https://openreview.net/pdf/8a9e97704fb73192eb6b4659e20f8a0203c51ce1.pdf"><img src="resources/assets/img/cleans/cleans_arxiv.jpg" width="100%"></a></p>
	       	<!-- <p><a target="_blank"><img src="resources/assets/img/cbunet/cbunet_arxiv.png" width="100%"></a></p> -->
	    </div>
		<div class="title-info-pynet">
            <br/>
            <br/>
			<p>TL;DR: <span style="color: red;">Noisy Raw (14.5M 40dB)</span> &rarr; <span style="color: red;">Clean Raw (0.1M 50dB) without</span> the need of the noise-clean <span style="color: red;">paired data</span>.</p>
            <br/>
	    	<p> <span style="font-weight:bold">Abstract:</span> Raw images offer unique advantages in many lowlevel visual tasks due to their unprocessed nature. However, this unprocessed state accentuates noise, making raw images challenging to compress effectively. Current compression methods often overlook the ubiquitous noise in raw space, leading to increased bitrates and reduced quality. In this paper, we propose a novel raw image compression scheme that selectively compresses the noise-free component of the input, while discarding its real noise using a self-supervised approach. By excluding noise from the bitstream, both the coding efficiency and reconstruction quality are significantly enhanced. We curate a full-day dataset of raw images with calibrated noise parameters and reference images to evaluate the performance of models under a wide range of input signalnoise ratios. Experimental results demonstrate that our method surpasses existing compression techniques, achieving a more advantageous ratedistortion balance with improvements ranging from +2 to +10dB and yielding a bit saving of 2 to 50 times.</p>
	        <!-- <p style="margin-top: 0.2vw;"> -->
	        	<!-- <a target="_blank" href="https://arxiv.org/pdf/2204.08970.pdf"> arXiv: 2204. </a> -->
	        	<!-- <a target="_blank" href="https://arxiv.org/pdf/1910.06663.pdf"><button class="follow-up"> Deep Learning on Smartphones Paper >> </button></a> -->
	        	<!--<a target="_blank" href="http://phancer.com/"><button class="follow-up" style="padding-left: 2.9vw; padding-right: 2.9vw; background-color: #f16b59;"> online demo </button></a>-->
	        <!-- </p> -->
		</div>
	</div>
</div>

<div id="demo">

	<h1 class="title"> <text class="highlight-color-blue">&#60;</text> Rendering Results <text class="highlight-color-blue">&#62;</text> </h1>

	<h1 class="title" style="font-size: 1.4vw;">
		Sony a7S II:&nbsp;&nbsp; <text class="highlight-color">Ours</text>&nbsp; vs. &nbsp;<text class="highlight-color-blue">RAW </text>
	</h1>
	
	<figure class="cd-image-container">
		<img src="resources/assets/img/SID/10030_00_0.1s.ARW_lq.png" alt="Original Image">
		<span class="cd-image-label" data-type="original">RAW</span>
		
		<div class="cd-resize-img">
			<img src="resources/assets/img/SID/10030_00_0.1s.ARW_result.png" alt="Modified Image">
			<span class="cd-image-label" data-type="modified">Ours</span>
		</div>
		
		<span class="cd-handle cd-first"></span>
	</figure>

	<figure class="cd-image-container">
		<img src="resources/assets/img/SID/10030_00_0.1s.ARW_lq.png" alt="Original Image">
		<span class="cd-image-label" data-type="original">RAW</span>
		
		<div class="cd-resize-img">
			<img src="resources/assets/img/SID/10030_00_0.1s.ARW_result.png" alt="Modified Image">
			<span class="cd-image-label" data-type="modified">Ours</span>
		</div>
		
		<span class="cd-handle cd-first"></span>
	</figure>

	<h1 class="title" style="font-size: 1.4vw;">
		Redmi Note12 Turbo:&nbsp;&nbsp; <text class="highlight-color">Ours</text>&nbsp; vs. &nbsp;<text class="highlight-color-blue">RAW </text>
	</h1>
	
    <figure class="cd-image-container">
		<img src="resources/assets/img/OV64B/noise_true_875_30000000_2024_01_01_19_36_30_942.dng_lq.png" alt="Original Image">
		<span class="cd-image-label" data-type="original">RAW</span>
		
		<div class="cd-resize-img">
			<img src="resources/assets/img/OV64B/noise_true_875_30000000_2024_01_01_19_36_30_942.dng_result.png" alt="Modified Image">
			<span class="cd-image-label" data-type="modified">CBUnet</span>
		</div>
		
		<span class="cd-handle cd-first"></span>
	</figure>

    <figure class="cd-image-container">
		<img src="resources/assets/img/OV64B/noise_true_875_30000000_2024_01_01_19_36_30_942.dng_lq.png" alt="Original Image">
		<span class="cd-image-label" data-type="original">RAW</span>
		
		<div class="cd-resize-img">
			<img src="resources/assets/img/OV64B/noise_true_875_30000000_2024_01_01_19_36_30_942.dng_result.png" alt="Modified Image">
			<span class="cd-image-label" data-type="modified">CBUnet</span>
		</div>
		
		<span class="cd-handle cd-first"></span>
	</figure>

</div>

<div id="method">

	<h1 class="title"> <text class="italic"> <text class="highlight-color">&lt;</text> Method </text>&nbsp;<text class="highlight-color">&gt;</text> </h1>

	<p><img src="resources/assets/img/cleans/framework.png" width="80%; margin-top: 1.2vw;"></p>

	<div>
		<div class="camsdd-info">
            The proposed framework for self-supervised raw image denoising and compression without reliance on paired clean images. Distinct from typical learning-based compressors, our approach first subtracts fixed pattern noise \( n_{fp} \) from the noise input \( \tilde{x} \) in the compressor encoder. Then, it compresses the predicted clean signal \( \hat{x} \), constrained by \( \tilde{x} - F_n(\tilde{x}; \Omega) \), using a compressor with an integrated hyperprior module. To regularize the predicted noise \( \hat{n} = F_n(\tilde{x}; \Omega) \), we use a bijective mapping based on the physical-based noise model to map the complicated noise distribution to a latent space where the distribution is known. Besides, a covariance loss is used to enhance the spatial independence of the disentangled noise \( \hat{n} \).
		</div>
	</div>
</div>
<!-- <div id="demo">

	<h1 class="title"> <text class="highlight-color-blue">&#60;</text> Method <text class="highlight-color-blue">&#62;</text> </h1>

	<figure style="width: 80%; margin: 0 auto; text-align: left;">
		<img src="resources/assets/img/OV64B/noise_true_875_30000000_2024_01_01_19_36_30_942.dng_lq.png" alt="Original Image" style="width: 100%; height: auto;">
		<figcaption>The proposed framework for self-supervised raw image denoising and compression without reliance on paired clean images.
			Distinct from typical learning-based compressors, our approach first subtracts fixed pattern noise nf p from the noise input  ̃x in the
			compressor encoder. Then, it compresses the predicted clean signal ˆx, constrained by  ̃x − Fn( ̃x; Ω), using a compressor with an integrated
			hyperprior module. To regularize the predicted noise ˆn = Fn( ̃x; Ω), we use a bijective mapping based on the physical-based noise model
			to map the complicated noise distribution to a latent space where the distribution is known. Besides, a covariance loss is used to enhance
			the spatial independence of the disentangled noise ˆn.</figcaption>
	</figure>

</div> -->

<div id="dataset">

    <h1 class="title"> <text class="highlight-color-blue">&#60;</text> Full Day Raw Image Compression Dataset <text class="highlight-color-blue">&#62;</text> </h1>

	<div>
		<div class="zrr-image">
			<p><img src="resources/assets/img/FDRIC/noise_everywhere.png" width="100%"></p>
			
			<br/>

	       	<p><img src="resources/assets/img/FDRIC/FDRIC.png" width="100%"></p>
	    </div>
		<div class="zrr-info">
	    	<p> Existing raw image denoising datasets mainly focus on lowlight or nighttime scenes, e.g., SID (Chen et al., 2018) captured under around 5 lux conditions and ELD (Wei et al., 2020) is even below 0.3 lux as shown in Fig. 4a. However, the demand for image compression is not only at night but throughout the whole day. Besides, noise is also prevalent in daylight raw images, which possess considerably higher SNR. Due to the significant gap among input SNR, compressors trained solely on low SNR data are less effective in higher SNR scenarios. Therefore, the existing datasets cannot well meet the needs of training and performance evaluation.
	    	</p>
	    	<!-- <br/> -->
	    	<br/>
			<p>
			To address this limitation, we develop a full-day raw image compression (FDRIC) dataset that covers a wide range of SNR, ensuring comprehensive training and evaluation. We collected our dataset using the Redmi Note12 Turbo smartphone, with an OV64B sensor of a resolution of 4624×3472. Our dataset includes 549 noisy images for training and 32 noise-clean image pairs for evaluation. Our dataset contains indoor and outdoor scenes with illumination ranging from 0.1 to more than 100,000 lux.
			</p>
	    	<!-- <br/> -->
	    	<br/>
			<p>
			<!-- It should be mentioned that all alignment operations were performed only on RGB DSLR images, therefore RAW photos from Huawei P20 remained unmodified, containing the same values as were obtained from the camera sensor. -->
			<!-- </p> -->
			<!-- <br/> -->
			<!-- <br/> -->
			<a target="_blank" href=""><button class="zrr-btn-download">Full Day Raw Image Compression Dataset (11.97 GB)</button></a>
		</div>
	</div>
	

</div>

<!-- <div id="algo">

	<h1 class="title"> PyNET Architecture </h1>
	
		<div class="pynet-architecture-info">
			<p>
			PyNET model has an inverted pyramidal shape and is processing the images at <text class="highlight-color">five different scales</text>. The proposed architecture has a number of blocks that are processing feature maps in parallel with convolutional filters of different size (from 3&#215;3 to 9&#215;9), and the outputs of the corresponding convolutional layers are then concatenated, which allows the network to learn a more diverse set of features at each level. The outputs obtained at lower scales are upsampled with transposed convolutional layers, stacked with feature maps from the upper level and then subsequently processed in the following convolutional layers. <text class="italic">Leaky ReLU</text> activation function is applied after each convolutional operation, except for the output layers that are using <text class="italic">Tanh</text> function to map the results to (-1, 1) interval. Instance normalization is used in all convolutional layers that are processing images at lower scales (levels 2-5).
			</p>
			<br/>
			<br/>
			<p>
			The model <text class="highlight-color">is trained sequentially</text>, starting from the lowest layer. This allows to achieve good image reconstruction results at smaller scales that are working with images of very low resolution and performing mostly global image manipulations. After the bottom layer is pre-trained, the same procedure is applied to the next level till the training is done on the original resolution. Since each higher level is getting upscaled high-quality features from the lower part of the model, it mainly learns to reconstruct the missing low-level details and refines the results. Note that the input layer is always the same and is getting images of size 224&#215;224&#215;4, though only a part of the training graph (all layers participating in producing the outputs at the corresponding scale) is trained.
			</p>
		</div>
		<div class="pynet-architecture-image">
	       	<p><img src="assets/img/pynet/architecture_pynet.png" width="100%"></p>
	    </div>

</div> -->

<div id="code">
	<h1 class="title"> <text class="highlight-color-blue">&#60;</text> Code <text class="highlight-color-blue">&#62;</text> </h1>
	<!-- <p><text class="highlight-color-blue">TensorFlow</text> PyNET implementation and the entire training pipeline is available in our <a target="_blank" class="grey-link" href="https://github.com/aiff22/PyNET">github repository</a></p> -->
	<br/>
	<p><text class="highlight-color-blue">PyTorch</text> implementation and pre-trained models can be found <a target="_blank" class="grey-link" href="https://github.com/lizhihao6/Cleans">here</a></p>
</div>

<div id="cite">
	<h1 class="title"> <text class="highlight-color-blue">&#60;</text> Citation <text class="highlight-color-blue">&#62;</text> </h1>
    <div style="border: 1px solid #ccc; padding: 10px; margin: 10px 0; background-color: #f9f9f9; line-height: 1.6; text-align: left;">
        @inproceedings{
		<br>
		&nbsp;&nbsp;Cleans,<br>
        &nbsp;&nbsp;title=<em>Compress Clean Signal from Noisy Raw Image: A Self-Supervised Approach</em>,<br>
        &nbsp;&nbsp;author={Li, Zhihao and Wang, Yufei and Kot, Alex and Wen, Bihan},<br>
        &nbsp;&nbsp;booktitle={Forty-first International Conference on Machine Learning}<br>
        }
    </div>
	
		<!-- <p><text class="italic">Zhihao Li, Si Yi </text> and <text class="italic">Zhan Ma.</text></p>
		<p>"Rendering Nighttime Image Via Cascaded Color and Brightness Compensation",</p>
		<p><text class="italic">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</text>, 2022</p> -->
		<!--<br/>
		<br/>
		<br/>
		<p><text class="small-text">The work is supported by the ETH Zurich General Fund, Toyota via the project TRACE-Zurich, and an NVidia GPU grant.</text></p>-->

</div>

<div id="about">
	<div id="content">
		<!-- <p>Contact: {ihnatova, nk, timofter, vanhoey}@vision.ee.ethz.ch</p> -->
		<!-- <p>Vision Lab, Nanjing University</p> -->
		<p>*The theme of the website was borrowed from <a href="http://people.ee.ethz.ch/~ihnatova/pynet.html">PyNET</a>.</p>
		<!-- <p>Switzerland, 2020-2021</p> -->
	</div>
</div>

<script src="resources/js/jquery-2.1.1.js"></script>
<script src="resources/js/jquery.mobile.custom.min.js"></script>
<script src="resources/js/main.js"></script>
<script>
	function open_demo() {
		document.getElementById('demos').style.display="block";
		document.getElementById('demo-button').style.display="none";
	}
	function close_demo() {
		document.getElementById('demos').style.display="none";
		document.getElementById('demo-button').style.display="block";
	}
</script>
</body>

</html>
